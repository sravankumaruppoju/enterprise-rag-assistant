ğŸš€ Enterprise RAG Assistant (v2.0)

A production-ready Retrieval-Augmented Generation (RAG) system built using Streamlit, LangChain, OpenAI, and Chroma.

This application allows users to securely upload multiple PDFs and ask contextual questions powered by OpenAI embeddings and LLMs.

â¸»

ğŸ”— Live Demo

ğŸ‘‰ Add your Streamlit URL here
ğŸ· Latest Release: v2.0

â¸»

ğŸ“Œ Overview

Enterprise RAG Assistant is a secure multi-document question-answering system designed with real-world GenAI architecture principles.

The system:
	â€¢	Accepts multiple PDF documents
	â€¢	Chunks and embeds text using OpenAI embeddings
	â€¢	Stores vectors in Chroma
	â€¢	Retrieves relevant context dynamically
	â€¢	Generates grounded answers using OpenAI LLMs
	â€¢	Provides source references (file + page)
	â€¢	Enforces password protection for secure access

This is not a simple chatbot â€” it is an enterprise-grade RAG implementation

ğŸ§  System Architecture
User
  â†“
Streamlit UI
  â†“
LangChain Orchestration
  â†“
Chroma Vector Store
  â†“
OpenAI Embeddings
  â†“
OpenAI LLM (gpt-4o-mini / gpt-4.1-mini)

Processing Flow
	1.	User uploads one or more PDFs
	2.	Text is chunked using RecursiveCharacterTextSplitter
	3.	Embeddings are generated via OpenAI
	4.	Chunks are stored in Chroma vector DB
	5.	Top-K relevant chunks are retrieved per query
	6.	LLM generates answer strictly from retrieved context
	7.	Sources are displayed for transparency

â¸»

âœ¨ Key Features

ğŸ” Secure Access
	â€¢	Password protection
	â€¢	Secure API key handling via Streamlit Secrets
	â€¢	Environment-based configuration

ğŸ“‚ Multi-PDF Support
	â€¢	Upload multiple PDFs simultaneously
	â€¢	Automatic indexing refresh
	â€¢	SHA256-based caching to avoid duplicate embeddings

âš™ï¸ Configurable Retrieval
	â€¢	Adjustable Top-K chunk retrieval
	â€¢	Adjustable temperature
	â€¢	Model selection (gpt-4o-mini / gpt-4.1-mini)

ğŸ§  Hallucination Reduction

The prompt enforces:

If the answer is not present in the context, respond with:
â€œI donâ€™t know based on the uploaded document.â€

This ensures grounded responses.

ğŸš€ Production-Ready Design
	â€¢	Cached embeddings & LLM
	â€¢	Clean Git branching workflow (v2 â†’ main)
	â€¢	Versioned release tagging
	â€¢	Cloud deployment on Streamlit

â¸»

ğŸ›  Tech Stack
	â€¢	Python
	â€¢	Streamlit
	â€¢	LangChain
	â€¢	OpenAI (Embeddings + LLM)
	â€¢	Chroma (Vector Database)
	â€¢	dotenv
	â€¢	SHA256 hashing (document fingerprinting)

â¸»

ğŸ” Engineering Highlights

1ï¸âƒ£ Recursive Chunking

Improves semantic retrieval quality over simple character splitting.

2ï¸âƒ£ Vector Caching

Prevents re-embedding identical documents using SHA256 fingerprinting.

3ï¸âƒ£ Strict Context Prompting

Minimizes hallucination by restricting answers to retrieved context only.

4ï¸âƒ£ Resource Caching

@st.cache_resource used for:
	â€¢	LLM initialization
	â€¢	Embedding model
	â€¢	Vector store build

Improves performance and scalability.

â¸»

ğŸ“Š Scalability Path

This architecture can easily evolve to:
	â€¢	Persistent vector storage (Pinecone / Weaviate)
	â€¢	Hybrid retrieval (BM25 + Vector search)
	â€¢	FastAPI backend for production
	â€¢	Role-based access control
	â€¢	RAG evaluation (RAGAS)
	â€¢	Streaming LLM responses
	â€¢	LangSmith observability integration

  ğŸ¯ Use Cases
	â€¢	Enterprise knowledge base assistant
	â€¢	Legal document analysis
	â€¢	Healthcare policy QA
	â€¢	Financial document retrieval
	â€¢	Resume & compliance document parsing

â¸»

ğŸ‘¤ Author

Sravan Kumar Uppoju
Senior Data Scientist | GenAI Engineer

â¸»

â­ Why This Project Stands Out

This project demonstrates:
	â€¢	Real-world RAG architecture
	â€¢	Secure GenAI system design
	â€¢	Vector database integration
	â€¢	Prompt engineering for hallucination control
	â€¢	Cloud deployment workflow
	â€¢	Version control & release management

It reflects production-level AI engineering practices.
